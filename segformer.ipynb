{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 09:39:29.657858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-3,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':16,\n",
    "        'epochs':500,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path='../../data/external/ori/*.png'\n",
    "mask1_path='../../data/external/mask/class1/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = Image.open(self.img_path[idx])\n",
    "        image_path=tf(image_path)\n",
    "        file_path=os.path.basename(self.img_path[idx])\n",
    "        label1 = np.array(Image.open(self.label[idx]))\n",
    "        label1=label1[:,:,0,np.newaxis]\n",
    "        label2=np.array(Image.open(self.label[idx].replace('/class1', '/class2')))\n",
    "        label2=label2[:,:,0,np.newaxis]\n",
    "        label3=np.array(Image.open(self.label[idx].replace('/class1', '/class3')))\n",
    "        label3=label3[:,:,0,np.newaxis]\n",
    "\n",
    "        label=np.concatenate((label1,label2,label3),axis=2)\n",
    "        label_path = tf(cv2.resize(label, (512, 512)))\n",
    "       \n",
    "        return image_path, label_path,file_path\n",
    "\n",
    "test_dataset = CustomDataset(glob(image_path), glob(mask1_path))\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=params['batch_size'],shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at matei-dorian/segformer-b5-finetuned-human-parsing and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([18]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([18, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] =(2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return dice_per_class\n",
    "\n",
    "def compute_iou(pred_mask, true_mask, threshold=0.5, num_classes=3):\n",
    "    \"\"\"\n",
    "    IoU를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :return: IoU 값\n",
    "    \"\"\"\n",
    "    iou_per_class = torch.zeros(num_classes).to(device)\n",
    "    for class_id in range(num_classes):\n",
    "    # 예측된 마스크 이진화\n",
    "        pred_mask1 = (pred_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_mask1 = (true_mask[:,class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 교차 계산\n",
    "        intersection = torch.sum(pred_mask1 * true_mask1)\n",
    "        \n",
    "        # 합집합 계산\n",
    "        union = torch.sum(pred_mask1) + torch.sum(true_mask1) - intersection\n",
    "        \n",
    "        # IoU 계산\n",
    "        iou_per_class[class_id]= intersection / union\n",
    "    \n",
    "    return iou_per_class\n",
    "\n",
    "def compute_f1(pred_mask, true_mask, threshold=0.5, num_classes=3, device='cpu'):\n",
    "    \"\"\"\n",
    "    F1 점수를 계산하는 함수\n",
    "\n",
    "    :param pred_mask: 모델이 예측한 마스크 (torch.Tensor)\n",
    "    :param true_mask: 실제 마스크 (torch.Tensor)\n",
    "    :param threshold: 이진화를 위한 임계값\n",
    "    :param num_classes: 클래스의 수\n",
    "    :param device: 연산에 사용할 디바이스 (기본값: 'cpu')\n",
    "    :return: 각 클래스별 F1 점수, Precision, Recall, Specificity, Accuracy (torch.Tensor)\n",
    "    \"\"\"\n",
    "    f1_per_class = torch.zeros(num_classes).to(device)\n",
    "    precision1 = torch.zeros(num_classes).to(device)\n",
    "    recall1 = torch.zeros(num_classes).to(device)\n",
    "    specificity1 = torch.zeros(num_classes).to(device)\n",
    "    accuracy1 = torch.zeros(num_classes).to(device)\n",
    "    \n",
    "    for class_id in range(num_classes):\n",
    "        # 예측된 마스크 이진화\n",
    "        pred_binary_mask = (pred_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # 실제 마스크 이진화\n",
    "        true_binary_mask = (true_mask[:, class_id, ...] > threshold).float()\n",
    "        \n",
    "        # True Positive (TP), False Positive (FP), False Negative (FN), True Negative (TN) 계산\n",
    "        TP = torch.sum(pred_binary_mask * true_binary_mask)\n",
    "        FP = torch.sum(pred_binary_mask * (1 - true_binary_mask))\n",
    "        FN = torch.sum((1 - pred_binary_mask) * true_binary_mask)\n",
    "        TN = torch.sum((1 - pred_binary_mask) * (1 - true_binary_mask))\n",
    "        \n",
    "        # 정밀도 (Precision) 계산\n",
    "        precision = TP / (TP + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 재현율 (Recall) 계산\n",
    "        recall = TP / (TP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 특이도 (Specificity) 계산\n",
    "        specificity = TN / (TN + FP + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # 정확도 (Accuracy) 계산\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        \n",
    "        # F1 점수 계산\n",
    "        f1_per_class[class_id] = 2 * (precision * recall) / (precision + recall + 1e-8)  # 분모가 0이 되는 것을 방지하기 위해 작은 값 추가\n",
    "        precision1[class_id] = precision.item()\n",
    "        recall1[class_id] = recall.item()\n",
    "        specificity1[class_id] = specificity.item()\n",
    "        accuracy1[class_id] = accuracy.item()\n",
    "    \n",
    "    return f1_per_class, precision1, recall1, specificity1, accuracy1\n",
    "\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\",num_labels=3,ignore_mismatched_sizes=True).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_Step: 17:   6%|▋         | 16/256 [00:13<03:19,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m f1,precision,recall,specificity, accuracy\u001b[38;5;241m=\u001b[39mcompute_f1(predict, y)\n\u001b[1;32m     25\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(df)]\u001b[38;5;241m=\u001b[39m[file_path[\u001b[38;5;241m0\u001b[39m],cost[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),cost[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),cost[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),cost\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),iou[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),iou[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),iou[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),iou\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),f1\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),precision\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),recall\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),specificity\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),accuracy\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()]\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/external/result/segformer/k_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/label/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m transform(torch\u001b[38;5;241m.\u001b[39mwhere(predict[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/external/result/segformer/k_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pred/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mfile_path[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     28\u001b[0m test\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_Step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/Image.py:2470\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   2469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n\u001b[0;32m-> 2470\u001b[0m     \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = T.ToPILImage()\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('../../model/segformer/seg_former_'+str(i+1)+'.pth',map_location=device))\n",
    "    df=pd.DataFrame(columns=['file_name','Dice1','Dice2','Dice3','mDice','IoU1','IoU2','IoU3','mIoU','f1','precision','sensitivity','specificity','accuracy'])\n",
    "    with torch.no_grad():\n",
    "        test = tqdm(test_dataloader)\n",
    "        count = 0\n",
    "        val_running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y,file_path in test:\n",
    "            model.eval()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            output =model(x).logits.cpu()\n",
    "            predict = nn.functional.interpolate(\n",
    "                    output,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "            ).to(device)\n",
    "            cost = dice_loss(predict, y)  # cost 구함\n",
    "            iou=compute_iou(predict, y)\n",
    "            f1,precision,recall,specificity, accuracy=compute_f1(predict, y)\n",
    "            \n",
    "            df.loc[len(df)]=[file_path[0],cost[0].item(),cost[1].item(),cost[2].item(),cost.mean().item(),iou[0].item(),iou[1].item(),iou[2].item(),iou.mean().item(),f1.mean().item(),precision.mean().item(),recall.mean().item(),specificity.mean().item(),accuracy.mean().item()]\n",
    "            transform(y[0].cpu()).save('../../data/external/result/segformer/k_'+str(i+1)+'/label/'+file_path[0])\n",
    "            transform(torch.where(predict[0]>0.5,1,0).cpu().float()).save('../../data/external/result/segformer/k_'+str(i+1)+'/pred/'+file_path[0])\n",
    "            test.set_description(\n",
    "                f\"val_Step: {count+1}\")\n",
    "    df.to_csv('../../data/external/result/segformer/segformer_'+str(i+1)+'_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 09:50:24.428455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at matei-dorian/segformer-b5-finetuned-human-parsing and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([18]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([18, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "epoch: 1/300 Step: 9 iou_loss : 0.9962 iou_score: 0.0038:   1%|          | 8/1000 [00:15<31:14,  1.89s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m cost \u001b[38;5;241m=\u001b[39m iou_loss(predict, y)  \u001b[38;5;66;03m# cost 구함\u001b[39;00m\n\u001b[1;32m    138\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mcost\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 139\u001b[0m \u001b[43mcost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# cost에 대한 backward 구함\u001b[39;00m\n\u001b[1;32m    140\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    141\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m cost\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n",
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':16,\n",
    "        'epochs':100,}\n",
    "image1=np.ones((4000,512,512,1))\n",
    "image1=image1.astype(np.uint8)\n",
    "# image2=np.load('../../data/cv1_ori.npy')\n",
    "# image2=image2.astype(np.uint8)\n",
    "# image3=np.load('../../data/cv2_ori.npy')\n",
    "# image3=image3.astype(np.uint8)\n",
    "# image4=np.load('../../data/cv3_ori.npy')\n",
    "# image4=image4.astype(np.uint8)\n",
    "# image5=np.load('../../data/cv4_ori.npy')\n",
    "# image5=image5.astype(np.uint8)\n",
    "mask1=np.ones((4000,512,512,4))\n",
    "mask1=(mask1[:,:,:,:3]).astype(np.uint8)\n",
    "# mask2=np.load('../../data/cv1_mask.npy')\n",
    "# mask2=(mask2[:,:,:,:3]).astype(np.uint8)\n",
    "# mask3=np.load('../../data/cv2_mask.npy')\n",
    "# mask3=(mask3[:,:,:,:3]).astype(np.uint8)\n",
    "# mask4=np.load('../../data/cv3_mask.npy')\n",
    "# mask4=(mask4[:,:,:,:3]).astype(np.uint8)\n",
    "# mask5=np.load('../../data/cv4_mask.npy')\n",
    "# mask5=(mask5[:,:,:,:3]).astype(np.uint8)\n",
    "\n",
    "np_data={'image1':image1,'image2':image1,'image3':image1,'image4':image1,'image5':image1,'mask1':mask1,'mask2':mask1,'mask3':mask1,'mask4':mask1,'mask5':mask1}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path, (512, 512)))\n",
    "       \n",
    "        return image_path, label_path\n",
    "    \n",
    "def dice_loss(pred, target, num_classes=3):\n",
    "    smooth = 1.\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] = 1 - \\\n",
    "            (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return torch.mean(dice_per_class)\n",
    "\n",
    "def iou_loss(pred, target, num_classes=3):\n",
    "    smooth = 1e-6\n",
    "    iou_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        union = torch.sum(pred_class) + torch.sum(target_class) - intersection\n",
    "\n",
    "        iou_per_class[class_id] = 1 - (intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return torch.mean(iou_per_class)\n",
    "    \n",
    "metrics = defaultdict(float)\n",
    "for k in range(2,5):\n",
    "    val_loss=1000\n",
    "    df=pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "    train_list=[0,1,2,3,4]\n",
    "    train_list.remove(k)\n",
    "    train_image=np.concatenate([np_data['image'+str(i+1)] for i in train_list])\n",
    "    train_mask=np.concatenate([np_data['mask'+str(i+1)] for i in train_list])\n",
    "    val_image=np_data['image'+str(k+1)]\n",
    "    val_mask=np_data['mask'+str(k+1)]\n",
    "    train_dataset = CustomDataset(train_image, train_mask)\n",
    "\n",
    "    val_dataset = CustomDataset(val_image, val_mask)\n",
    "    train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    validation_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    ealry_count=0\n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(\"matei-dorian/segformer-b5-finetuned-human-parsing\",num_labels=3,ignore_mismatched_sizes=True).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "    for epoch in range(300):\n",
    "        train = tqdm(train_dataloader)\n",
    "        count = 0\n",
    "        running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        \n",
    "        for x, y in train:\n",
    "            model.train()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "            predict = model(x).logits.to(device)\n",
    "            predict = nn.functional.interpolate(\n",
    "                predict,\n",
    "                size=(512,512),\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            cost = iou_loss(predict, y)  # cost 구함\n",
    "            acc = 1-cost.item()\n",
    "            cost.backward()  # cost에 대한 backward 구함\n",
    "            optimizer.step()\n",
    "            running_loss += cost.item()\n",
    "            acc_loss += acc\n",
    "            train.set_description(\n",
    "                f\"epoch: {epoch+1}/{300} Step: {count+1} iou_loss : {running_loss/count:.4f} iou_score: {1-running_loss/count:.4f}\")\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            val = tqdm(validation_dataloader)\n",
    "            count = 0\n",
    "            val_running_loss = 0.0\n",
    "            acc_loss = 0\n",
    "            for x, y in val:\n",
    "                model.eval()\n",
    "                y = y.to(device).float()\n",
    "                count += 1\n",
    "                x = x.to(device).float()\n",
    "                predict = model(x).logits.to(device)\n",
    "                predict = nn.functional.interpolate(\n",
    "                    predict,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "                )\n",
    "                cost = iou_loss(predict, y)  # cost 구함\n",
    "                acc = 1-cost.item()\n",
    "                val_running_loss += cost.item()\n",
    "                acc_loss += acc\n",
    "\n",
    "                val.set_description(\n",
    "                    f\"val_epoch: {epoch+1}/{300} Step: {count+1} iou_loss : {val_running_loss/count:.4f} iou_score: {1-val_running_loss/count:.4f}\")\n",
    "        if val_loss>val_running_loss/count:\n",
    "            ealry_count=0\n",
    "            val_loss=val_running_loss/count\n",
    "            torch.save(model.state_dict(), '../../model/segformer/seg_former_'+str(k+1)+'_check.pth')\n",
    "        else:\n",
    "            ealry_count+=1\n",
    "            if epoch>10 and ealry_count==3:\n",
    "                break\n",
    "        df.loc[len(df)]=[epoch+1,running_loss/len(train_dataloader),val_running_loss/len(validation_dataloader),1-running_loss/len(train_dataloader),1-val_running_loss/len(validation_dataloader)]\n",
    "        df.to_csv('../../model/segformer/seg_former_'+str(k+1)+'.csv',index=False)\n",
    "    torch.save(model.state_dict(), '../../model/segformer/seg_former_'+str(k+1)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_GRAY2RGB\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<1>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 3\n"
     ]
    }
   ],
   "source": [
    "cv2.cvtColor(image1[1], cv2.COLOR_GRAY2RGB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
