{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 16:24:22.978330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':16,\n",
    "        'epochs':300,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=np.load('../../data/cv0_ori.npy')\n",
    "# image2=np.load('../../data/cv1_ori.npy')\n",
    "# image3=np.load('../../data/cv2_ori.npy')\n",
    "# image4=np.load('../../data/cv3_ori.npy')\n",
    "# image5=np.load('../../data/cv4_ori.npy')\n",
    "mask1=np.load('../../data/cv0_mask.npy')\n",
    "# mask2=np.load('../../data/cv1_mask.npy')\n",
    "# mask3=np.load('../../data/cv2_mask.npy')\n",
    "# mask4=np.load('../../data/cv3_mask.npy')\n",
    "# mask5=np.load('../../data/cv4_mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=np.zeros((40000,512,512,1),dtype=np.uint8)\n",
    "mask1=np.zeros((40000,512,512,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data={'image1':image1,'image2':image1,'image3':image1,'image4':image1,'image5':image1,'mask1':mask1,'mask2':mask1,'mask3':mask1,'mask4':mask1,'mask5':mask1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path, (128, 128)))\n",
    "       \n",
    "        return image_path, label_path\n",
    "\n",
    "train_dataset = CustomDataset(image1, mask1)\n",
    "\n",
    "val_dataset = CustomDataset(image1, mask1)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at mattmdjaga/segformer_b2_clothes and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([18]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([18, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def dice_loss(pred, target, num_classes=4):\n",
    "    smooth = 1.\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] = 1 - \\\n",
    "            (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return torch.mean(dice_per_class)\n",
    "\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\",num_labels=4,ignore_mismatched_sizes=True).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_data={'image1':image1,'image2':image1,'image3':image1,'image4':image1,'image5':image1,'mask1':mask1,'mask2':mask1,'mask3':mask1,'mask4':mask1,'mask5':mask1}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path, (512, 512)))\n",
    "       \n",
    "        return image_path, label_path\n",
    "    \n",
    "def dice_loss(pred, target, num_classes=4):\n",
    "    smooth = 1.\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] = 1 - \\\n",
    "            (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return torch.mean(dice_per_class)\n",
    "\n",
    "\n",
    "metrics = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at mattmdjaga/segformer_b2_clothes and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([18]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([18, 768, 1, 1]) in the checkpoint and torch.Size([4, 768, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "epoch: 1/300 Step: 76 dice_loss : 1.0000 dice_score: 0.0000:   3%|▎         | 75/2500 [00:58<31:39,  1.28it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(5):\n",
    "    val_loss=1000\n",
    "    df=pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'train_acc', 'val_acc'])\n",
    "    train_list=[0,1,2,3,4]\n",
    "    train_list.remove(k)\n",
    "    train_image=np_data['image1']\n",
    "    train_mask=np_data['mask1']\n",
    "    val_image=np_data['image1']\n",
    "    val_mask=np_data['mask1']\n",
    "    train_dataset = CustomDataset(image1, mask1)\n",
    "\n",
    "    val_dataset = CustomDataset(image1, mask1)\n",
    "    train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    validation_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "    \n",
    "    model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\",num_labels=4,ignore_mismatched_sizes=True).to(device)\n",
    "    optimizer = optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n",
    "    for epoch in range(1):\n",
    "        train = tqdm(train_dataloader)\n",
    "        count = 0\n",
    "        running_loss = 0.0\n",
    "        acc_loss = 0\n",
    "        for x, y in train:\n",
    "            model.train()\n",
    "            y = y.to(device).float()\n",
    "            count += 1\n",
    "            x = x.to(device).float()\n",
    "            optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "            output =model(x).logits.cpu()\n",
    "            predict = nn.functional.interpolate(\n",
    "                    output,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "            ).to(device)\n",
    "            cost = dice_loss(predict, y)  # cost 구함\n",
    "            acc = 1-cost.item()\n",
    "            cost.backward()  # cost에 대한 backward 구함\n",
    "            optimizer.step()\n",
    "            running_loss += float(cost)\n",
    "                \n",
    "            acc_loss += acc\n",
    "            train.set_description(\n",
    "                f\"epoch: {epoch+1}/{300} Step: {count+1} dice_loss : {running_loss/count:.4f} dice_score: {1-running_loss/count:.4f}\")\n",
    "            del output\n",
    "        with torch.no_grad():\n",
    "            val = tqdm(validation_dataloader)\n",
    "            count = 0\n",
    "            val_running_loss = 0.0\n",
    "            acc_loss = 0\n",
    "            for x, y in val:\n",
    "                model.eval()\n",
    "                y = y.to(device).float()\n",
    "                count += 1\n",
    "                x = x.to(device).float()\n",
    "                output =model(x).logits.cpu()\n",
    "                predict = nn.functional.interpolate(\n",
    "                    output,\n",
    "                    size=(512,512),\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False,\n",
    "                ).to(device)\n",
    "                cost = dice_loss(predict, y)\n",
    "                acc = 1-cost.item()\n",
    "                val_running_loss += float(cost)\n",
    "                del output\n",
    "                acc_loss += acc\n",
    "                val.set_description(\n",
    "                    f\"val_epoch: {epoch+1}/{300} Step: {count+1} dice_loss : {val_running_loss/count:.4f} dice_score: {1-val_running_loss/count:.4f}\")\n",
    "        \n",
    "                if val_loss>val_running_loss/count:\n",
    "                    val_loss=val_running_loss/count\n",
    "                    torch.save(model.state_dict(), '../../model/segformer/seg_former_'+str(k+1)+'_check.pth')\n",
    "        df.loc[len(df)]=[epoch+1,running_loss/len(train_dataloader),val_running_loss/len(validation_dataloader),1-running_loss/len(train_dataloader),1-val_running_loss/len(validation_dataloader)]\n",
    "        df.to_csv('../../model/segformer/seg_former_'+str(k+1)+'.csv',index=False)\n",
    "    torch.save(model.state_dict(), '../../model/segformer/seg_former_'+str(k+1)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
