{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "tf=T.ToTensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'image_size':512,\n",
    "        'lr':2e-4,\n",
    "        'beta1':0.5,\n",
    "        'beta2':0.999,\n",
    "        'batch_size':16,\n",
    "        'epochs':500,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1=np.load('../../data/cv0_ori.npy')\n",
    "# image2=np.load('../../data/cv1_ori.npy')\n",
    "# image3=np.load('../../data/cv2_ori.npy')\n",
    "# image4=np.load('../../data/cv3_ori.npy')\n",
    "# image5=np.load('../../data/cv4_ori.npy')\n",
    "mask1=np.load('../../data/cv0_mask.npy')\n",
    "# mask2=np.load('../../data/cv1_mask.npy')\n",
    "# mask3=np.load('../../data/cv2_mask.npy')\n",
    "# mask4=np.load('../../data/cv3_mask.npy')\n",
    "# mask5=np.load('../../data/cv4_mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, label_list):\n",
    "        self.img_path = image_list\n",
    "        self.label = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.img_path[idx]\n",
    "        image_path=tf(cv2.cvtColor(image_path, cv2.COLOR_GRAY2RGB))\n",
    "        \n",
    "        label_path = self.label[idx]\n",
    "        label_path = tf(cv2.resize(label_path, (128, 128)))\n",
    "       \n",
    "        return image_path, label_path\n",
    "\n",
    "train_dataset = CustomDataset(image1, mask1)\n",
    "\n",
    "val_dataset = CustomDataset(image1, mask1)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset, batch_size=params['batch_size'], shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, num_classes=4):\n",
    "    smooth = 1.\n",
    "    dice_per_class = torch.zeros(num_classes).to(pred.device)\n",
    "\n",
    "    for class_id in range(num_classes):\n",
    "        pred_class = pred[:, class_id, ...]\n",
    "        target_class = target[:, class_id, ...]\n",
    "\n",
    "        intersection = torch.sum(pred_class * target_class)\n",
    "        A_sum = torch.sum(pred_class * pred_class)\n",
    "        B_sum = torch.sum(target_class * target_class)\n",
    "\n",
    "        dice_per_class[class_id] = 1 - \\\n",
    "            (2. * intersection + smooth) / (A_sum + B_sum + smooth)\n",
    "\n",
    "    return torch.mean(dice_per_class)\n",
    "\n",
    "model = smp.MAnet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=4,                      # model output channels (number of classes in your dataset)\n",
    ").to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=params['lr'], betas=(params['beta1'], params['beta2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================================================================================================\n",
       "Layer (type:depth-idx)                                                                                    Output Shape              Param #\n",
       "===========================================================================================================================================================\n",
       "Mask2FormerForUniversalSegmentation                                                                       [1, 100, 256]             --\n",
       "├─Mask2FormerModel: 1-1                                                                                   [1, 100, 128, 128]        --\n",
       "│    └─Mask2FormerPixelLevelModule: 2-1                                                                   [1, 256, 16, 16]          --\n",
       "│    │    └─SwinBackbone: 3-1                                                                             [1, 96, 128, 128]         48,838,602\n",
       "│    │    └─Mask2FormerPixelDecoder: 3-2                                                                  [1, 256, 128, 128]        5,421,504\n",
       "│    └─Mask2FormerTransformerModule: 2-2                                                                  [100, 1, 256]             51,968\n",
       "│    │    └─Mask2FormerSinePositionEmbedding: 3-3                                                         [1, 256, 16, 16]          --\n",
       "│    │    └─Mask2FormerSinePositionEmbedding: 3-4                                                         [1, 256, 32, 32]          --\n",
       "│    │    └─Mask2FormerSinePositionEmbedding: 3-5                                                         [1, 256, 64, 64]          --\n",
       "│    │    └─Mask2FormerMaskedAttentionDecoder: 3-6                                                        [100, 1, 256]             14,406,656\n",
       "├─Linear: 1-2                                                                                             [1, 100, 81]              20,817\n",
       "├─Linear: 1-3                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-4                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-5                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-6                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-7                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-8                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-9                                                                                             [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-10                                                                                            [1, 100, 81]              (recursive)\n",
       "├─Linear: 1-11                                                                                            [1, 100, 81]              (recursive)\n",
       "===========================================================================================================================================================\n",
       "Total params: 68,739,547\n",
       "Trainable params: 68,739,547\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.93\n",
       "===========================================================================================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 2122.91\n",
       "Params size (MB): 265.08\n",
       "Estimated Total Size (MB): 2391.14\n",
       "==========================================================================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(1, 3, 512, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "metrics = defaultdict(float)\n",
    "for epoch in range(300):\n",
    "    train = tqdm(train_dataloader)\n",
    "    count = 0\n",
    "    running_loss = 0.0\n",
    "    acc_loss = 0\n",
    "    for x, y in train:\n",
    "        model.train()\n",
    "        y = y.to(device).float()\n",
    "        count += 1\n",
    "        x = x.to(device).float()\n",
    "        optimizer.zero_grad()  # optimizer zero 로 초기화\n",
    "        predict = model(x).logits.to(device)\n",
    "        cost = dice_loss(predict, y)  # cost 구함\n",
    "        acc = 1-dice_loss(predict, y)\n",
    "        cost.backward()  # cost에 대한 backward 구함\n",
    "        optimizer.step()\n",
    "        running_loss += cost.item()\n",
    "        acc_loss += acc\n",
    "        y = y.to('cpu')\n",
    "\n",
    "        x = x.to('cpu')\n",
    "        train.set_description(\n",
    "            f\"epoch: {epoch+1}/{300} Step: {count+1} dice_loss : {running_loss/count:.4f} dice_score: {1-running_loss/count:.4f}\")\n",
    "    train_loss_list.append((running_loss/count))\n",
    "    train_acc_list.append((acc_loss/count).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x).logits.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
